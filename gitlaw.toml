[model]
# Model Name
file_name = "phi-2.gguf"
# Model Location
path = "./file"
# Phi-2 Temp
#| Temp        | Behavior                                                |
#| ----------- | ------------------------------------------------------- |
#| `0.0`       | üîí Totally deterministic (always gives same answer)     |
#| `0.2 ‚Äì 0.5` | ‚úçÔ∏è Reasonable, focused writing                          |
#| `0.7 ‚Äì 1.0` | üåÄ More creative, looser summaries                      |
#| `> 1.0`     | ‚ö†Ô∏è Chaotic, unpredictable (not recommended for commits) |
temperature = 0.7

[engine]
# Optional: full path to compiled binary
# e.g., from `llama.cpp/main` or bundled `gitlaw-engine`
path = "./file"
file_name = "llama.cpp"

# url path
[download]
# Direct URLs to download dependencies if needed
engine = "https://github.com/ggml-org/llama.cpp/releases/download/b5476/llama-b5476-bin-ubuntu-x64.zip"
model = "https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf?download=true"